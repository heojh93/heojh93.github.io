---
layout: page
title:  "Expectation Maximization"
date:   2018-05-03 00:00:00
category: 인공지능
tags: Expectation-Maximization
---


## Hidden Variable을 찾아서

아래와 같이 A,B 두개의 class에 값들이 담겨있는데, 하나의 값이 없다. 이 데이터를 어떻게 찾아낼까?
![em0]({{site.url}}/assets/artificial/em0.png){:height="90px" .center-image}


---

## 어떤 값을 넣는게 좋은지 어떻게 판단하지?

모수(parameter)로부터 특정 현상이 얼마나 관찰되는지를 표현한 것이 확률이라고 한다. 반대로 어떤 현상이 주어졌을 때, 어떤 모수에서 이 현상이 발생할 가능성을 **우도(likelihood)**라고 한다.  

$$f$$ 는 확률밀도함수 혹은 확률질량함수이고, $$\,X = (x_1, x_2, ... , x_n)$$는 우리가 관찰한 값이다. $$\,\theta$$는 $$f$$의 parameter이다. 예를 들어, $$\,f$$가 normal distribution이라고 하면, $$\,\theta$$는 $$\,f$$가 가질 수 있는 parameter인 $$\,\mu$$나 $$\,\sigma$$의 정보를 갖고 있는 값이 될 것이다. 이제 아래 그림을 보면 대충 이해가 갈 것이다.  

![em1]({{site.url}}/assets/artificial/em1.png){:height="400px" .center-image}

< probablity와 likelihood의 비교 >
{: style="text-align: center;"}

확률밀도 함수이므로, probablity의 경우 0이다.


---

## Maximum Likelihood Estimation


Hidden Variable을 찾기 위해서 likelihood를 높게 하도록 하는 $$\theta$$을 찾아가면 된다. 이를 **MLE(Maximum Likelihood Estimation)**이라고 하며 다음과 같이 수식으로 표현한다.

$$\hat{\theta} = arg \, max_{\theta} \mathcal{L}(\theta;X) = arg \, max_{\theta}\,f(X \mid \theta)$$


---

## EM 알고리즘

결국 EM 알고리즘이란, MLE를 푸는 방법이다. 다음과 같이 진행한다.

* initial $$\theta$$를 고름  
* **E-step** : Expectation 단계로, 새로운 $$\theta$$에 대한 $$H$$의 기댓값을 계산한다.

$$P_{t+1}(H) = Pr(H \mid D, \theta{t})$$  

* **M-step** : log-likelihood를 최대로 만드는 새로운 $$\theta$$를 계산한다. log를 취하게 되면 곱셈이 덧셈으로 바뀌게 되므로, 계산의 편리를 위해 log를 사용한다.

$$\theta_{t+1} = argmax_{\theta}\,E_{P_{t+1}} logPr(D,H \mid \theta)$$


* 수렴할 때 까지 E-step과 M-step을 반복.

---

## example

알고리즘을 참고하여, 처음에 제시된 문제를 해결해보자!

**1. $$\theta_0$$을 가정하였다.**

A, B가 uniform하게 분포되어있다고 가정하였다.

![em_example0]({{site.url}}/assets/artificial/em_x0.png){:height="90px" .center-image}

$$\theta_0$$로부터 log-likelihood를 구해보면, $$\, logPr(D \mid \theta_1) = -10.3972$$이다.

**2. $$\theta_0$$으로 부터 $$H$$의 기댓값을 구한다.**

\begin{align} Pr(H \mid D, \theta_0) & = Pr(B \mid \neg A, \theta_0) \\\\ 
& = Pr(\neg A, B \mid \theta_0) \, / \, Pr(\neg A \mid \theta_0) \\\\ 
& = 0.25 / 0.5 \\\\ 
& = 0.5 \end{align}

이는 $$P(H=0) = 0.5, \, P(H=1) = 0.5$$임을 의미한다. 

**3. 예측된 $$H$$를 이용하여 새로 $$\theta_1$$을 구한다.**  

![em_example1]({{site.url}}/assets/artificial/em_x1.png){:height="90px" .center-image}

구해진 $$\theta_1$$로부터 log-likelihood를 구해보면, $$\, logPr(D \mid \theta_1) = -9.4760$$이다.

**4. 2~3을 반복한다.**

몇 회 반복하지 않아서 $$\theta$$는 수렴하는 값을 갖게 된다.

![em_example2]({{site.url}}/assets/artificial/em_x2.png){:height="90px" .center-image}

이 때, $$logPr(D \mid \theta_n) = -9.4514$$이며, $$\,P(H=0) = 0.74, \, P(H=1) = 0.26$$이다.

---

## Bayesian Network에서의 EM 알고리즘

H 없이 A,B 만 있는 상태에서 A,B를 Bayesian Network라고 보고 H를 추론할 수 있다. A,B의 CPT(Conditional Probability Table)가 아래와 같이 주어졌다고 하자. H는 현재 빈 값이다. $$P(H^m \mid D^m, \theta_t)$$는 A,B의 pair와 $$\theta$$가 주어졌을 때, 해당 확률로 H가 1을 갖는다는 것을 의미한다.

![em_bn]({{site.url}}/assets/artificial/em_bn.png){:height="200px" .center-image}


처음에 $$\theta$$를 다음과 같이 가정하자.  
>
$$Pr(H) = 0.4$$  
$$Pr(A \mid H) = 0.55$$  
$$Pr(A \mid \neg H) = 0.61$$  
$$Pr(B \mid H) = 0.43$$  
$$Pr(B \mid \neg H) = 0.52$$  
>

이 $$\theta$$를 이용하여 $$P(H^m \mid D^m, \theta_t)$$를 채워 넣을 수 있다. 예를 들어, $$P(H^3 \mid D^3, \theta_0)$$는 다음과 같이 구할 수 있다.  
\begin{align} Pr(H^3 \mid A, B)
& = \frac{Pr(A,B \mid H^3) \cdot Pr(H^3)}{Pr(A,B)} \\\\ 
& = \frac{Pr(A \mid H^3) \cdot Pr(A \mid H^3) \cdot Pr(H^3)}{Pr(A \mid H^3) \cdot Pr(A \mid H^3) \cdot Pr(H^3) + Pr(A \mid \neg H^3) \cdot Pr(A \mid \neg H^3) \cdot Pr(\neg H^3)} \\\\ 
& = 0.477 \end{align}

CPT를 채워넣었다면, 이 값들을 이용하여 $$\theta$$를 다시 계산하는 것이 가능하고, 반복적으로 계산을 하다보면 어느정도 수렴하는 점에 이르르게 된다.

EM알고리즘의 E-step, M-step을 10번 반복했을 때, $$P(H^0 \mid D^0, \theta_{10}) = 0.971$$, $$P(H^3 \mid D^3, \theta_{10}) = 0.001$$이 된다.

---

참고:  
http://sragent.tistory.com/entry/EM-튜토리얼  
http://dermabae.tistory.com/188  
https://www.youtube.com/watch?v=XepXtl9YKwc  (maximum likelihood)  
한양대학교 인공지능 강의자료  


