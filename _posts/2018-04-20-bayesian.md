---
layout: page
title:  "Naive Bayesian Classifier"
date:   2018-04-20 00:00:00
category: 인공지능
tags: Naive-Bayesian-Classifier
---

classifier 2. Naive Bayesian Classifier

<!-- more -->

---

### Bayesian Theorem

조건부확률을 중고등학교때 배웠다.

$$P(A|B) = \frac{P(A \cap B)}{P(B)}$$

$$P(B|A) = \frac{P(A \cap B)}{P(A)}$$

이를 공통인 $$P(A \cap B)$$ 를 이용해서 식을 정리하면 다음과 같은 식이 나온다.

$$P(A|B) = \frac{P(B \mid A) \cdot P(A)}{P(B)}$$

---

실제로 한번 예시를 들어서 식을 사용해보자.

>
병 H를 진단하는 검사가 있다고 하자.  
검사의 정밀도는 98%이다. 즉, 실제 H에 걸렸을 때 이 검사가 양성반응(+)을 보일 확률은 98%이다.
1%의 확률로 병이 없을 때 양성반응(+)을 보인다.  
H의 발병률은 0.2%이다.  
이 검사가 병 H에 대해 양성반응을 보였을 때, 실제 H에 걸렸을 확률은?
>

$$P(H)$$ = 0.002, $$P(+ \mid H)$$ = 0.98, $$P(+ \mid \neg H)$$ = 0.01 이다.

$$P(+)$$ = $$P(H)P(+ \mid H)+P(\neg H)P(+ \mid \neg H)$$ = 0.012이고,  

$$P(H \mid +) = \frac{P(+ \mid H) P(+)}{P(H)}$$ 이다. 계산을 해보면, $$P(H \mid +)$$=0.16, 즉 16%가 된다.  

병을 거의 정확히 진단하는 검사를 하였는데, 검사결과를 신뢰할 수 있는 확률은 16%밖에 안된다는 아이러니함을 볼 수 있다.

---

### Naive Bayesian Classifier


**1. independence**

A와 B가 독립이라면, $$P(A \cap B) = P(A) \cdot P(B)$$ 이 성립한다.

이에따라, 다음이 성립한다.

$$P(A \mid B) = P(A)$$  
$$P(B \mid A) = P(B)$$

<br/>

**2. conditional independence**

집합이란 개념을 "어떤 조건에 의해 걸러진 원소들"이라고 생각해보자.  

$$P(A \mid C)$$는 조건 C를 만족하는 원소 중 조건 A를 만족하는 원소를 찾는 것과 같다.  
$$P(A \mid C,B)$$는 조건 C,B를 만족하는 원소 중 조건 A를 만족하는 원소를 찾는 것과 같다.

그렇다면, $$P(A \mid C) = P(A \mid C,B)$$라면 어떤 경우일까?

조건 C로 인해 한번 걸러진 원소들 중에서 조건 B로 한번 더 걸러봐야 A를 찾는 데 별 의미가 없음을 의미한다. 따라서 이 경우를 `A is conditionally independent of B given C`라고 하며 $$A \perp\!\!\!\perp B \mid C$$라고 쓴다. 


C가 주어졌을 때, A와 B가 독립이라면 다음이 성립한다.


\begin{align} P(A,B \mid C) & = \frac{P(A,B,C)}{P(C)} \\\\ & = \frac{P(A,B,C)}{P(B,C)} \cdot \frac{P(B,C)}{P(C)} \\\\ & = P(A \mid B,C) \cdot P(B \mid C) \\\\ & = P(A \mid C) \cdot P(B \mid C) \end{align}


Naive Bayesian Classifier는 이를 이용한다.

<br/>

**3. Naive Bayesian Classifier**

$$P(H|X) = \frac{P(X \mid H) \cdot P(H)}{P(X)}$$


* $$X=(x_1,x_2,...,x_n)$$ : 데이터 샘플
* $$H$$ : X가 특정 class에 속한다는 가설(Hypothesis)
* $$P(H)$$ : 특정 클래스의 빈도 (prior probability)
* $$P(X \mid H)$$ : 특정 클래스에 X가 나올 확률 (posteriori probability)
* $$P(H \mid X)$$ : 데이터 샘플 X가 H 클래스 속할 확률 (likelyhood)

각 클래스$$C_i$$마다 데이터 샘플 $$X$$의 likelyhood를 계산해서, 그 값이 가장 높은 클래스를 선택하게 된다.
likelyhood는 클래스 $$C_i$$가 주어졌을 때 $$x_1, x_2, ..., x_n$$이 독립이라고 가정하고 계산한다.
이 가정이 붙기 때문에 Naive Bayesian Classifier이다. $$P(X)$$는 상수이므로, 비교할 때 굳이 필요가 없어서 아래처럼 계산을 하면 된다.

$$P(H \mid X) = P(H \mid x_1,x_2,...x_n) = \sum_{k=1}^{m}P(x_k \mid C_i)$$

---

### Naive Bayesian Classifier 구현

앞선 글의 input과 동일한 training set을 받는다. [dataset]({{site.url}}/asset/artificial/dt_train1.txt)의 링크이다.

{% highlight python linenos %}
import sys
from collections import defaultdict

def beyesian(rules, attr, dataCnt, classCnt):
	
	max = 0 
	size = len(dataCnt)
	raplacian = 0
	for c in classCnt:
		pxc = 1

		for idx in range(len(rules)):
			rule = makeRule(attr[idx], rules[idx])

			if(dataCnt.get(rule,0) == 0):
				raplacian = 1
		
			pxc *= (float(dataCnt[rule][c] + raplacian) / (classCnt[c] + raplacian*(size)))

		pxc *= classCnt[c]

		if(max < pxc):
			max = pxc
			rst = c
			
	i = rst.index('=')

	return rst[i+1:]


def makeRule(attr, value):
	return attr + "=" + value


# attr : {0 : "attr0", 1 : "attr1", ... }
# classCnt : { class1 : n1, class2 : n2, ... }
# P(X|C), dataCnt['attr=value']['class'] = count
def makeDataSet(dataSet):

	attrTmp = dataSet.readline()[:-1].split('\t')
	attr = defaultdict(int)
	for i in range(len(attrTmp)):
		attr[i] = attrTmp[i]
	
	classIdx = len(attr) - 1
	classCnt = defaultdict(int)
	
	dataCnt = defaultdict(lambda: defaultdict(int))

	for line in dataSet:
		items = line[:-1].split('\t')

		classified = makeRule(attr[classIdx], items[classIdx])
		classCnt[classified] += 1

		for idx in range(len(items)-1):
			rule = makeRule(attr[idx], items[idx])
			dataCnt[rule][classified] += 1

	return dataCnt, classCnt, attr

if __name__ == "__main__":
	trainingSet = open(sys.argv[1],"r")
	testSet = open(sys.argv[2], "r")
	output = open(sys.argv[3], "w")

	dataCnt, classCnt, attr = makeDataSet(trainingSet)

	a = beyesian(['<=40', 'medium', 'yes', 'fair'], attr, dataCnt, classCnt)
	print(a)
{% endhighlight %}
